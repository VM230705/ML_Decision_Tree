{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "934b83ed",
   "metadata": {},
   "source": [
    "# Dataset: sentiment-analysis-on-movie-reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5aa8034",
   "metadata": {},
   "source": [
    "The raw-data can be downloaded from http://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data. </p>\n",
    "First, let's explore the dataset using pandas. The columns of the dataset are tab-delimited. The dataset contains 156060 instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab7c0d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "# import nltk\n",
    "# nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import pad_sequences\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='bs4')\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "102f456d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path):\n",
    "    filename, extension = os.path.splitext(path)\n",
    "\n",
    "    if extension == '.csv':\n",
    "        rawdata = pd.read_csv(path, header=0, delimiter=',')\n",
    "    elif extension == '.xlsx':\n",
    "        rawdata = pd.read_excel(path, header=0)\n",
    "    elif extension == '.tsv':\n",
    "        rawdata = pd.read_csv(path, header=0, delimiter='\\t')\n",
    "    return rawdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "351d037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    reviews = []\n",
    "    for raw in tqdm(df['Phrase']):\n",
    "        text = BeautifulSoup(raw, 'lxml').get_text()\n",
    "        only_text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "        words = word_tokenize(only_text.lower())\n",
    "        stops = set(stopwords.words('english'))\n",
    "        non_stopwords = []\n",
    "        #non_stopwords = [word for word in words if not word in stops]\n",
    "        for word in words:\n",
    "            if not word in stops:\n",
    "                non_stopwords.append(word)\n",
    "            else:\n",
    "                pass\n",
    "        lemma_words = [lemmatizer.lemmatize(word) for word in non_stopwords]    \n",
    "        reviews.append(lemma_words)\n",
    "#         print(\"text  = \", text)\n",
    "#         print(\"only = \", only_text)\n",
    "#         print(\"words = \", words)\n",
    "#         print(\"stops = \", stops)\n",
    "#         print(\"non stops = \", non_stopwords)\n",
    "#         print(\"lemma = \", lemma_words)\n",
    "#         print(\"-------------------------------\")\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6fb102e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_preprocess(list_X_train, list_X_val):\n",
    "    unique_words = set()\n",
    "    len_max = 0\n",
    "    for sent in tqdm(list_X_train):\n",
    "        unique_words.update(sent)\n",
    "        if len_max < len(sent):\n",
    "            len_max = len(sent)\n",
    "    len(list(unique_words)), len_max\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=len(list(unique_words)))\n",
    "    tokenizer.fit_on_texts(list(list_X_train))\n",
    "     \n",
    "    X_train = tokenizer.texts_to_sequences(list_X_train)\n",
    "    X_train = pad_sequences(X_train, maxlen=len_max)\n",
    "\n",
    "    X_val = tokenizer.texts_to_sequences(list_X_val)\n",
    "    X_val = pad_sequences(X_val, maxlen=len_max)\n",
    "\n",
    "    return X_train, X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2eca902a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(predictions, y_test):\n",
    "    print('Accuracy: %s' % accuracy_score(y_test, predictions))\n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion_matrix(y_test, predictions))\n",
    "    print('Classification Report:')\n",
    "    print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "870466a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "    \n",
    "    def is_leaf(self):\n",
    "        return self.value is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a2f20e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None, min_samples_split=5, method=\"entropy\"):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.method = method\n",
    "        self.root = None\n",
    "        self.classes = None\n",
    "\n",
    "    def _is_finished(self, depth, parent_samples):\n",
    "        \"\"\"Check if stop to grow or not.\"\"\"\n",
    "        if (self.max_depth is not None and depth >= self.max_depth\n",
    "            or self.classes == 1\n",
    "            or self.n_samples < self.min_samples_split\n",
    "            or self.n_samples == parent_samples\n",
    "            or self.n_samples == 0):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        \"\"\"Calculate the entropy of a given set of labels.\"\"\"\n",
    "        proportions = np.bincount(y) / len(y)\n",
    "        entropy = -np.sum([p * np.log2(p) for p in proportions if p > 0])\n",
    "        return entropy\n",
    "\n",
    "    def _gini_index(self, y):\n",
    "        \"\"\"Calculate the Gini index of a given set of labels.\"\"\"\n",
    "        proportions = np.bincount(y) / len(y)\n",
    "        gini = 1 - np.sum([p*p for p in proportions if p > 0])\n",
    "        return gini\n",
    "\n",
    "    def _create_split(self, X, thresh):\n",
    "        \"\"\"Create a split in the data based on a given threshold.\"\"\"\n",
    "        left_idx = np.argwhere(X <= thresh).flatten()\n",
    "        right_idx = np.argwhere(X > thresh).flatten()\n",
    "        return left_idx, right_idx\n",
    "\n",
    "    def _information_gain(self, X, y, thresh):\n",
    "        \"\"\"Calculate the information gain from splitting on a given feature and threshold.\"\"\"\n",
    "        left_idx, right_idx = self._create_split(X, thresh)\n",
    "        n, n_left, n_right = len(y), len(left_idx), len(right_idx)\n",
    "\n",
    "        if n_left == 0 or n_right == 0: \n",
    "                return 0\n",
    "        if self.method == \"gini\":\n",
    "            parent_loss = self._gini_index(y)\n",
    "            child_loss = (n_left / n) * self._gini_index(y[left_idx]) + (n_right / n) * self._gini_index(y[right_idx])\n",
    "        else:\n",
    "            parent_loss = self._entropy(y)\n",
    "            child_loss = (n_left / n) * self._entropy(y[left_idx]) + (n_right / n) * self._entropy(y[right_idx])\n",
    "        return parent_loss - child_loss\n",
    "\n",
    "    def _best_split(self, X, y, features):\n",
    "        split = {'score':- 1, 'feat': None, 'thresh': None}\n",
    "        \n",
    "        for feat in features:\n",
    "            X_feat = X[:, feat]\n",
    "            thresholds = np.unique(X_feat)\n",
    "            for thresh in thresholds:\n",
    "                score = self._information_gain(X_feat, y, thresh)\n",
    "                \n",
    "                if score > split['score']:\n",
    "                    split['score'] = score\n",
    "                    split['feat'] = feat\n",
    "                    split['thresh'] = thresh\n",
    "    \n",
    "        return split['score'], split['feat'], split['thresh']\n",
    "    def _build_tree(self, X, y, depth=0, parent_samples=0):\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "        self.classes = len(np.unique(y))\n",
    "        \n",
    "        rnd_feats = np.random.choice(self.n_features, self.n_features, replace=False)\n",
    "        score, best_feat, best_thresh = self._best_split(X, y, rnd_feats)\n",
    "\n",
    "        if self._is_finished(depth, parent_samples) or score == 0:\n",
    "            most_common_Label = np.argmax(np.bincount(y))\n",
    "            return Node(value=most_common_Label)\n",
    "        left_idx, right_idx = self._create_split(X[:, best_feat], best_thresh)\n",
    "        left_child = self._build_tree(X[left_idx, :], y[left_idx], depth + 1, self.n_samples)\n",
    "        right_child = self._build_tree(X[right_idx, :], y[right_idx], depth + 1, self.n_samples)\n",
    "        return Node(best_feat, best_thresh, left_child, right_child)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.root = self._build_tree(X, y)\n",
    "\n",
    "    def _predict_one(self, x, node):\n",
    "        if node.is_leaf():\n",
    "            return node.value\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._predict_one(x, node.left)\n",
    "        return self._predict_one(x, node.right)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return [self._predict_one(x, self.root) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "20e0f4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "30dbc21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx = read_file('./Dataset2_train/X_train.xlsx')\n",
    "dfy = read_file('./Dataset2_train/y_train.xlsx')\n",
    "# dfx = read_file('./Dataset1_train/train/X_train.xlsx')\n",
    "# dfy = read_file('./Dataset1_train/train/y_train.xlsx')\n",
    "df = pd.concat([dfx, dfy], axis=1)\n",
    "df = df.dropna()  #drop missing value(phrase)\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ef734967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    63664\n",
      "3    26342\n",
      "1    21818\n",
      "4     7365\n",
      "0     5658\n",
      "Name: Sentiment, dtype: int64\n",
      "2    0.509936\n",
      "3    0.210994\n",
      "1    0.174758\n",
      "4    0.058992\n",
      "0    0.045319\n",
      "Name: Sentiment, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "descriptive_feature = dfx.columns\n",
    "target_feature = dfy.columns[0]\n",
    "print(df['Sentiment'].value_counts())\n",
    "print(df['Sentiment'].value_counts()/df['Sentiment'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "995c3d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 124847/124847 [00:38<00:00, 3239.47it/s]\n"
     ]
    }
   ],
   "source": [
    "X, y = dfx.values, dfy.values\n",
    "train_text = preprocess_data(df)\n",
    "target = df.Sentiment.values\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_text, target, test_size=0.2, stratify=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5c2b547c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 99877/99877 [00:00<00:00, 1203345.06it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_, X_val_ = tokenizer_preprocess(X_train, X_val)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7cf843a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['often', 'boring']\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  90 373]\n",
      "2\n",
      "30\n",
      "99877\n",
      "99877\n",
      "[1 2 3 ... 2 2 2]\n",
      "[ 4526 17454 50931 21074  5892]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])\n",
    "print(X_train_[0])\n",
    "print(len(X_train[0]))\n",
    "print(len(X_train_[0]))\n",
    "print(len(X_train))\n",
    "print(len(y_train))\n",
    "print(y_train)\n",
    "print(np.bincount(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "db9fb6a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5369243091710052\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTree()\n",
    "clf.fit(X_train_, y_train)\n",
    "y_pred = clf.predict(X_val_)\n",
    "acc = accuracy(y_val, y_pred)\n",
    "print(\"Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "81d2a4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5369243091710052\n",
      "Confusion Matrix:\n",
      "[[ 362  413  227   97   33]\n",
      " [ 499 1785 1578  426   76]\n",
      " [ 345 1878 8770 1519  221]\n",
      " [ 163  575 2013 2103  414]\n",
      " [  63  141  297  585  387]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.32      0.28      1132\n",
      "           1       0.37      0.41      0.39      4364\n",
      "           2       0.68      0.69      0.68     12733\n",
      "           3       0.44      0.40      0.42      5268\n",
      "           4       0.34      0.26      0.30      1473\n",
      "\n",
      "    accuracy                           0.54     24970\n",
      "   macro avg       0.42      0.42      0.41     24970\n",
      "weighted avg       0.54      0.54      0.54     24970\n",
      "\n",
      "[ 1432  4792 12885  4730  1131]\n",
      "[ 1132  4364 12733  5268  1473]\n"
     ]
    }
   ],
   "source": [
    "report(y_pred, y_val)\n",
    "print(np.bincount(y_pred))\n",
    "print(np.bincount(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "937c24fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n",
      "Accuracy: 0.5372446936323588\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train))\n",
    "print(type(y_train))\n",
    "clf = DecisionTree(max_depth=100)\n",
    "clf.fit(X_train_, y_train)\n",
    "y_pred = clf.predict(X_val_)\n",
    "acc = accuracy(y_val, y_pred)\n",
    "print(\"Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1d4ade8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5372446936323588\n",
      "Confusion Matrix:\n",
      "[[ 360  419  225   96   32]\n",
      " [ 496 1789 1583  420   76]\n",
      " [ 347 1875 8771 1523  217]\n",
      " [ 148  574 1998 2114  434]\n",
      " [  59  145  294  594  381]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      0.32      0.28      1132\n",
      "           1       0.37      0.41      0.39      4364\n",
      "           2       0.68      0.69      0.69     12733\n",
      "           3       0.45      0.40      0.42      5268\n",
      "           4       0.33      0.26      0.29      1473\n",
      "\n",
      "    accuracy                           0.54     24970\n",
      "   macro avg       0.42      0.42      0.41     24970\n",
      "weighted avg       0.54      0.54      0.54     24970\n",
      "\n",
      "[ 1410  4802 12871  4747  1140]\n",
      "[ 1132  4364 12733  5268  1473]\n"
     ]
    }
   ],
   "source": [
    "report(y_pred, y_val)\n",
    "print(np.bincount(y_pred))\n",
    "print(np.bincount(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6bf2807c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 31212/31212 [00:09<00:00, 3271.07it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████| 124847/124847 [00:00<00:00, 2061112.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1730  5945 16165  5888  1484]\n"
     ]
    }
   ],
   "source": [
    "testdf = read_file('./Dataset2_test/X_test.xlsx')\n",
    "test_data = preprocess_data(testdf)\n",
    "train, test = tokenizer_preprocess(train_text, test_data)\n",
    "clf = DecisionTree()\n",
    "clf.fit(train, target)\n",
    "test_prediction = clf.predict(test)\n",
    "\n",
    "output = pd.DataFrame (test_prediction)\n",
    "\n",
    "filepath = 'dataset2_pred.xlsx'\n",
    "output.columns = ['Sentiment']\n",
    "print(np.bincount(output['Sentiment']))\n",
    "output.to_excel(filepath, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
