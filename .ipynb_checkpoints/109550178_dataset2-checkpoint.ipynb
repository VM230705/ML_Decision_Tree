{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "934b83ed",
   "metadata": {},
   "source": [
    "# Dataset: sentiment-analysis-on-movie-reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5aa8034",
   "metadata": {},
   "source": [
    "The raw-data can be downloaded from http://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data. </p>\n",
    "First, let's explore the dataset using pandas. The columns of the dataset are tab-delimited. The dataset contains 156060 instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab7c0d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "# import nltk\n",
    "# nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import pad_sequences\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='bs4')\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "102f456d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path):\n",
    "    filename, extension = os.path.splitext(path)\n",
    "\n",
    "    if extension == '.csv':\n",
    "        rawdata = pd.read_csv(path, header=0, delimiter=',')\n",
    "    elif extension == '.xlsx':\n",
    "        rawdata = pd.read_excel(path, header=0)\n",
    "    elif extension == '.tsv':\n",
    "        rawdata = pd.read_csv(path, header=0, delimiter='\\t')\n",
    "    return rawdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "351d037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    reviews = []\n",
    "    for raw in tqdm(df['Phrase']):\n",
    "        text = BeautifulSoup(raw, 'lxml').get_text()\n",
    "        only_text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "        words = word_tokenize(only_text.lower())\n",
    "        stops = set(stopwords.words('english'))\n",
    "        non_stopwords = []\n",
    "        #non_stopwords = [word for word in words if not word in stops]\n",
    "        for word in words:\n",
    "            if not word in stops:\n",
    "                non_stopwords.append(word)\n",
    "            else:\n",
    "                pass\n",
    "        lemma_words = [lemmatizer.lemmatize(word) for word in non_stopwords]    \n",
    "        reviews.append(lemma_words)\n",
    "#         print(\"text  = \", text)\n",
    "#         print(\"only = \", only_text)\n",
    "#         print(\"words = \", words)\n",
    "#         print(\"stops = \", stops)\n",
    "#         print(\"non stops = \", non_stopwords)\n",
    "#         print(\"lemma = \", lemma_words)\n",
    "#         print(\"-------------------------------\")\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fb102e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_preprocess(list_X_train, list_X_val):\n",
    "    unique_words = set()\n",
    "    len_max = 0\n",
    "    for sent in tqdm(list_X_train):\n",
    "        unique_words.update(sent)\n",
    "        if len_max < len(sent):\n",
    "            len_max = len(sent)\n",
    "    len(list(unique_words)), len_max\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=len(list(unique_words)))\n",
    "    tokenizer.fit_on_texts(list(list_X_train))\n",
    "     \n",
    "    X_train = tokenizer.texts_to_sequences(list_X_train)\n",
    "    X_train = pad_sequences(X_train, maxlen=len_max)\n",
    "\n",
    "    X_val = tokenizer.texts_to_sequences(list_X_val)\n",
    "    X_val = pad_sequences(X_val, maxlen=len_max)\n",
    "\n",
    "    return X_train, X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2eca902a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(predictions, y_test):\n",
    "    print('Accuracy: %s' % accuracy_score(y_test, predictions))\n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion_matrix(y_test, predictions))\n",
    "    print('Classification Report:')\n",
    "    print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "870466a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "    \n",
    "    def is_leaf(self):\n",
    "        return self.value is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2f20e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None, min_samples_split=5, split_criterion=\"entropy\"):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.split_criterion  = split_criterion \n",
    "        self.root = None\n",
    "        self.classes = None\n",
    "        self.deepest = 0\n",
    "    \n",
    "    def _build_tree(self, X, y, depth=0, parent_samples=0):\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "        self.classes = len(np.unique(y))\n",
    "        if(depth > self.deepest): self.deepest = depth\n",
    "            \n",
    "         # randomly select features to consider for split\n",
    "        rnd_feats = np.random.choice(self.n_features, self.n_features, replace=False)\n",
    "         # find best split based on selected features\n",
    "        score, best_feat, best_thresh = self._best_split(X, y, rnd_feats)\n",
    "        \n",
    "         # Check if reach stop criteria\n",
    "        if self._is_finished(depth, parent_samples) or score == 0:\n",
    "            return self._leaf_value(y)\n",
    "        \n",
    "        # create children nodes and continue building tree recursively\n",
    "        left_idx, right_idx = self._create_split(X[:, best_feat], best_thresh)\n",
    "        left_child = self._build_tree(X[left_idx, :], y[left_idx], depth + 1, self.n_samples)\n",
    "        right_child = self._build_tree(X[right_idx, :], y[right_idx], depth + 1, self.n_samples)\n",
    "        return Node(best_feat, best_thresh, left_child, right_child)\n",
    "    \n",
    "    def _leaf_value(self, y):\n",
    "        return Node(value=np.argmax(np.bincount(y)))\n",
    "    \n",
    "    def _best_split(self, X, y, features):\n",
    "        split = {'score':- 1, 'feat': None, 'thresh': None}\n",
    "        \n",
    "        for feat in features:\n",
    "            X_feat = X[:, feat]\n",
    "            thresholds = np.unique(X_feat)\n",
    "            for thresh in thresholds:\n",
    "                score = self._information_gain(X_feat, y, thresh)\n",
    "                \n",
    "                if score > split['score']:\n",
    "                    split['score'] = score\n",
    "                    split['feat'] = feat\n",
    "                    split['thresh'] = thresh\n",
    "    \n",
    "        return split['score'], split['feat'], split['thresh']\n",
    "    \n",
    "    def _is_finished(self, depth, parent_samples):\n",
    "        \"\"\"Check if stop to grow or not.\"\"\"\n",
    "        if (self.max_depth is not None and depth >= self.max_depth\n",
    "            or self.classes == 1\n",
    "            or self.n_samples < self.min_samples_split\n",
    "            or self.n_samples == parent_samples\n",
    "            or self.n_samples == 0):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def _create_split(self, X, thresh):\n",
    "        \"\"\"Create a split in the data based on a given threshold.\"\"\"\n",
    "        left_idx = np.argwhere(X <= thresh).flatten()\n",
    "        right_idx = np.argwhere(X > thresh).flatten()\n",
    "        return left_idx, right_idx\n",
    "    \n",
    "    def _split_criterion(self, y):\n",
    "        proportions = np.bincount(y) / len(y)\n",
    "        if(self.split_criterion == 'gini'):\n",
    "            value = 1 - np.sum([p*p for p in proportions if p > 0])\n",
    "        else:\n",
    "            value = -np.sum([p * np.log2(p) for p in proportions if p > 0])\n",
    "        return value\n",
    "    \n",
    "    def _information_gain(self, X, y, thresh):\n",
    "        \"\"\"Calculate the information gain from splitting on a given feature and threshold.\"\"\"\n",
    "        left_idx, right_idx = self._create_split(X, thresh)\n",
    "        n, n_left, n_right = len(y), len(left_idx), len(right_idx)\n",
    "\n",
    "        if n_left == 0 or n_right == 0: \n",
    "                return 0\n",
    "        parent_loss = self._split_criterion(y)\n",
    "        child_loss = (n_left / n) * self._split_criterion(y[left_idx]) + (n_right / n) * self._split_criterion(y[right_idx])\n",
    "        return parent_loss - child_loss\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.root = self._build_tree(X, y)\n",
    "\n",
    "    def _predict_one(self, x, node):\n",
    "        if node.is_leaf():\n",
    "            return node.value\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._predict_one(x, node.left)\n",
    "        return self._predict_one(x, node.right)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return [self._predict_one(x, self.root) for x in X]\n",
    "    \n",
    "    def getdeepest(self):\n",
    "        return self.deepest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20e0f4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30dbc21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx = read_file('./Dataset2_train/X_train.xlsx')\n",
    "dfy = read_file('./Dataset2_train/y_train.xlsx')\n",
    "df = pd.concat([dfx, dfy], axis=1)\n",
    "df = df.dropna()  #drop missing value(phrase)\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef734967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    63664\n",
      "3    26342\n",
      "1    21818\n",
      "4     7365\n",
      "0     5658\n",
      "Name: Sentiment, dtype: int64\n",
      "2    0.509936\n",
      "3    0.210994\n",
      "1    0.174758\n",
      "4    0.058992\n",
      "0    0.045319\n",
      "Name: Sentiment, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "descriptive_feature = dfx.columns\n",
    "target_feature = dfy.columns[0]\n",
    "print(df['Sentiment'].value_counts())\n",
    "print(df['Sentiment'].value_counts()/df['Sentiment'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "995c3d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 124847/124847 [01:21<00:00, 1536.83it/s]\n"
     ]
    }
   ],
   "source": [
    "X, y = dfx.values, dfy.values\n",
    "train_text = preprocess_data(df)\n",
    "target = df.Sentiment.values\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_text, target, test_size=0.2, stratify=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c2b547c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 99877/99877 [00:00<00:00, 703953.87it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_, X_val_ = tokenizer_preprocess(X_train, X_val)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60a18136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8381709502688307\n",
      "Accuracy: 0.5350820985182219\n",
      "Confusion Matrix:\n",
      "[[ 377  403  216  106   30]\n",
      " [ 447 1776 1622  411  108]\n",
      " [ 323 1868 8804 1563  175]\n",
      " [ 146  563 2104 2029  426]\n",
      " [  42  130  314  612  375]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.33      0.31      1132\n",
      "           1       0.37      0.41      0.39      4364\n",
      "           2       0.67      0.69      0.68     12733\n",
      "           3       0.43      0.39      0.41      5268\n",
      "           4       0.34      0.25      0.29      1473\n",
      "\n",
      "    accuracy                           0.54     24970\n",
      "   macro avg       0.42      0.41      0.41     24970\n",
      "weighted avg       0.53      0.54      0.53     24970\n",
      "\n",
      "pred: [ 1335  4740 13060  4721  1114]\n",
      "label: [ 1132  4364 12733  5268  1473]\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTree()\n",
    "clf.fit(X_train_, y_train)\n",
    "train_pred = clf.predict(X_train_)\n",
    "train_acc = accuracy(train_pred, y_train)\n",
    "y_pred = clf.predict(X_val_)\n",
    "acc = accuracy(y_val, y_pred)\n",
    "print(\"Train Accuracy:\", train_acc)\n",
    "report(y_pred, y_val)\n",
    "print(\"pred:\", np.bincount(y_pred))\n",
    "print(\"label:\", np.bincount(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2b20459",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 31212/31212 [00:20<00:00, 1505.25it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████| 124847/124847 [00:00<00:00, 1002978.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1704  5885 16511  5666  1446]\n"
     ]
    }
   ],
   "source": [
    "testdf = read_file('./Dataset2_test/X_test.xlsx')\n",
    "test_data = preprocess_data(testdf)\n",
    "train, test = tokenizer_preprocess(train_text, test_data)\n",
    "test_prediction = clf.predict(test)\n",
    "output = pd.DataFrame (test_prediction)\n",
    "\n",
    "filepath = '109550178_dataset2_pred.xlsx'\n",
    "output.columns = ['Sentiment']\n",
    "print(np.bincount(output['Sentiment']))\n",
    "output.to_excel(filepath, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
